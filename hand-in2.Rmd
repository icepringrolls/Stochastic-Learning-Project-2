---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 2: Group 16"
author: "Weicheng Hua, Emil Johannesen Haugstvedt, Torbj√∏rn Baadsvik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("knitr")
library("rmarkdown")
```

\section{Problem 1}

```{r,eval=TRUE,echo=FALSE}
library(MASS)
str(Boston)
cehcl<-Boston
set.seed(1)
boston <- scale(Boston, center = T, scale = T)
train.ind = sample(1:nrow(boston), 0.8*nrow(boston))
boston.train = data.frame(boston[train.ind, ])
boston.test = data.frame(boston[-train.ind, ])
dim(boston.train)
```


\subsection{a)}
```{r,eval=TRUE,echo=TRUE}
library(leaps)
fwd_regfit <- regsubsets(medv~., data = boston.train, nvmax = 14, method = "forward" )
summary_fwd_regfit <- summary(fwd_regfit)
bck_regfit <- regsubsets(medv~., data = boston.train, nvmax = 14, method = "backward" )
summary_bck_regfit <- summary(bck_regfit)
par(mfrow=c(1,2))
plot(summary_fwd_regfit$rsq, xlab = "Number of Variables", ylab = "RSq fwd" ,type = "l")
plot(summary_bck_regfit$rsq, xlab = "Number of Variables", ylab = "RSq bck" ,type = "l")


```


\subsection{b)}

```{r,eval=TRUE,echo=TRUE}
number_predictors_selected <- 4
variables_fwd <- names(coef(fwd_regfit, id = number_predictors_selected))
variables_fwd
```
The four "predictors" from the forward stepwise selection are "rm" ,"dis", "ptratio" and "lstat".


\subsection{c)}

```{r,eval=TRUE,echo=TRUE}

library(glmnet)
set.seed(1)
x_train <- model.matrix(medv~., boston.train)[,-1]
y_train <- boston.train$medv

model_lass <- cv.glmnet(x_train, y_train, alpha =1, nfolds =4)
plot(model_lass)


best_lambda <- model_lass$lambda.min
best_lambda

coef_lass <- coef(model_lass, s = model_lass$lambda.1se )
coef_lass


```

(ii)
```{r,eval=TRUE,echo=TRUE}

best_lambda <- model_lass$lambda.min
best_lambda

```
Th best lambda value is given as:0.00315


(iii)
The fitted coefficients at the best $\lambda$ value is given by the function below. A plot for the coefficients value at different lambda is given as well. Indus is not a relevant coefficient at the lambda value with lowest MSE and the coefficient for age is very small small meaning that it is most likely significantly less important than the rest. This is possible to do for sclaed data. 

```{r,eval=TRUE,echo=TRUE}

coef_lass <- coef(model_lass, s = model_lass$lambda.min )
coef_lass

lambdas_to_try <- 10^seq(-7,1, length.out = 100)
res<- glmnet(x_train, y_train, alpha=1, lambda =lambdas_to_try ,standardize= FALSE)
plot(res, xvar= "lambda")
legend("bottomright", legend =colnames(x_train), cex =0.3)

```




\subsection{d)}
(i)True. Lasso is generally faster than step-wise especially when n(number of datapoints) and p(number of predictors) are very large and the number of relatively important predictors are small. This is because lasso can eliminate multiple predictors at once by increasing the value of lambda while stepwise can only eliminate it one by one. https://www.stat.cmu.edu/~ryantibs/papers/bestsubset.pdf

(ii) False.It is impossible for ridge regression to result in coefficients equal to zero. The coefficients approaches zero as lambda's value get extremely large. The coefficients can however become zero in Lasso regression.

(iii)False. Lasso is expected to perform better when there is a only relatively small number of important predictors and a significant proportion of unimportant coefficients that have very small or zero value. Ridge is expected to perform better when there is a high proportion of important predictors with predictors of all having roughly the same size.

(iv) Ture. The formula for Elastic Net is given as 

$\min _{\beta_{0}, \beta} \frac{1}{N} \sum_{i=1}^{N} w_{i} l\left(y_{i}, \beta_{0}+\beta^{T} x_{i}\right)+\lambda\left[(1-\alpha)\|\beta\|_{2}^{2} / 2+\alpha\|\beta\|_{1}\right]$

Where the $\alpha$ value can be varied between 0 and 1 to change the weighting between Ridge and Lasso.



\section{Problem 2}

```{r}
library(MASS)
set.seed(1)
# load a synthetic dataset
id <- "1CWZYfrLOrFdrIZ6Hv73e3xxt0SFgU4Ph" # google file ID
synthetic <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
# split into training and test sets
train.ind = sample(1:nrow(synthetic), 0.8 * nrow(synthetic)) 
synthetic.train = data.frame(synthetic[train.ind, ])
synthetic.test = data.frame(synthetic[-train.ind, ])
# show head(..)
# Y: response variable; X: predictor variable
head(synthetic)
```

\subsection{a)}
```{r,eval=TRUE,echo=TRUE}
library(pls)

pcr_model <- pcr(Y~., data =synthetic.train, scale =TRUE, validation ="CV")
validationplot(pcr_model, val.type ="MSEP")
title(sub ="PCR")


plsr_model<- plsr(Y~., data =synthetic.train, scale =TRUE, validation ="CV")
validationplot(plsr_model, val.type = "MSEP")
title(sub = "PLSR")

```



\subsection{b)}
The PCR method show an almost uniform reduction in MSEP with increasing number of principal components. On the other hand, PLSR showed a sharp drop in MSEP when moving from 0 to 1 component and from 3 to 4 components, with the MSEP approaching zero from 4 components and onwards. The main difference between the PCR and the PLSR method is that in PCR, the principal components are created without considering their significance to Y, while in PLSR the principal components are created such that each additional principal components is weighted to have less significance to the reponse variable.  


\section{Problem 3}

\subsection{a}

(i) True. The proof for this is pretty long and complicated. You can see it in the link

(ii) False. By increasing the number of cutpoints each step function will get more and more affected by the points within their range, and thus overfit.

(iii) False. The penalty term is \$\\int g''(t)\^2 dt\$.

(iv) True. With high \$k\$ the number more neighbors are needed in order to classify a point, thus the variance will be low and the bias will increase.

\subsection{b}

``` {r}
# Fit modelt
model <- gam(medv ~ rm + s(ptratio, k = 3) + poly(lstat, df = 2), data = boston.train)

# Plot model with training data
plot(model, boston.train)
```

\section{Problem 4}

\subsection{a)}

(i) False. The trees can handle interaction terms, but you can not specify them yourself. The tree will "find" them by its nature.

(ii) True. 

(iii) True.

(iv) True. It decides the number of "iterations" in the boosting algorithm.

\subsection{b)}


\subsection{c)}
First some given R code
```{r}
library(tidyverse)
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)

names(penguins) <- c("species","island","billL","billD","flipperL","mass","sex","year")

Penguins_reduced <- penguins %>%  
  dplyr::mutate(mass = as.numeric(mass),  
         flipperL = as.numeric(flipperL),
         year = as.numeric(year)) %>% 
  drop_na()

# We do not want "year" in the data (this will not help for future predictions)
Penguins_reduced <- Penguins_reduced[,-c(8)]

set.seed(4268)
# 70% of the sample size for training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)
train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```

(i)
```{r}
library(tree)

# Create the tree 
tree.penguin <- tree(species ~ .,
                     data = train,
                     split = 'gini')

# Plot the tree
plot(tree.penguin, type = 'uniform');text(tree.penguin)
```
(ii)
Now, apply 10-fold cross-validation 
```{r}
set.seed(123)

# Do 10-fold cross-validation
cv.penguin <- cv.tree(tree.penguin)

# Plot result from the 10-fold cross-validation
plot(cv.penguin)
```
(iii) 
From the above plot the optimal tree looks to be of size 4. 
```{r}

# Prune the tree according to the obseravtion in the above plot
prune_tree.penguin <- prune.misclass(tree.penguin, best = 4)

plot(prune_tree.penguin, type = 'uniform'); text(prune_tree.penguin)
```


```{r}
pred <- predict(prune_tree.penguin, test, type = 'class')
tab <- table(pred, test$species)
tab
```
The misclassification rate will then be:
```{r}
(1 - sum(diag(tab))/sum(tab))
```


\subsection{d)}
```{r}
library(gbm)
library(dismo)

#train.df <- data.frame(train)
#train.df['isAdelie'] <- train.df$species == 'Adelie'
#train.df['isChinstrap'] <- train.df$species == 'Chinstrap'
#train.df['isGentoo'] <- train.df$species == 'Gentoo'


#
#a <- gbm.step(data = train.df,
#              gbm.x = names(train.df[2:7]),
#              gbm.y = names(train.df[8:10]),
#              family = 'gaussian',
#              tolerance.method = 'auto')

boosted.penguin <- gbm(species~ .,
                       data = train,
                       n.trees = 500,
                       distribution = 'multinomial')

summary(boosted.penguin)
```

Now we predict on the test data and calculate the misclassification rate:

```{r}
pred <- predict.gbm(boosted.penguin, test, type = 'response')

pred.penguin <- colnames(pred)[apply(pred,1,which.max)]

tab.boost <- table(pred.penguin, test$species)
tab.boost
```
The misclassification rate is
```{r}
misclassification.rate.boost <- 1-sum(diag(tab.boost))/sum(tab.boost)
misclassification.rate.boost
```


\section{Problem 5}

\subsection{a)}

\subsection{b)}

\section{Problem 6}

\subsection{a)}

\subsection{b)}

\subsection{c)}

\subsection{d)}
