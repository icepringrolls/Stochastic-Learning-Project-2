---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 1: Group 16"
author: "Weicheng Hua, Emil Johannesen Haugstvedt, Torbjørn Baadsvik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("knitr")
library("rmarkdown")
```

\section{Problem 1}

\subsection{a)}

\subsection{b)}

\subsection{c)}

\subsection{d)}

\section{Problem 2}

\subsection{a)}

\subsection{b)}

\section{Problem 3}

\subsection{a}

(i) True. The proof for this is pretty long and complicated. You can see it in the link

(ii) False. By increasing the number of cutpoints each step function will get more and more affected by the points within their range, and thus overfit.

(iii) False. The penalty term is \$\\int g''(t)\^2 dt\$.

(iv) True. With high \$k\$ the number more neighbors are needed in order to classify a point, thus the variance will be low and the bias will increase.

\subsection{b}

``` {.{.{r}´´´}}}
# Fit model

model <- gam(medv ~ rm + s(ptratio, k = 3) + poly(lstat, df = 2), data = boston.train)

# Plot model with training data

plot(model, boston.train)
´´´´
```

\subsection{a)}

\subsection{b)}

\section{Problem 4}

\subsection{a)}

\subsection{b)}

\subsection{c)}

\subsection{d)}

\section{Problem 5}

\subsection{a)}

\subsection{b)}

\section{Problem 6}

\subsection{a)}

\subsection{b)}

\subsection{c)}

\subsection{d)}
