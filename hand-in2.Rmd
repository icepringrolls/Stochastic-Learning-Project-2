---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 2: Group 16"
author: "Weicheng Hua, Emil Johannesen Haugstvedt, Torbj√∏rn Baadsvik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE, warning=FALSE}
library("knitr")
library("rmarkdown")

# General additive models
library(mgcv)

# Classification
library(e1071)
library(caret)
library(kernlab)

# Partial least squares regression
library(plsRglm)
library(plsdof)

# K-means clustering
#install.packages(c("factoextra", "cluster"))
library(factoextra)
library(cluster)
```

\section{Problem 1}

```{r,eval=TRUE,echo=FALSE}
library(MASS)
str(Boston)
cehcl<-Boston
set.seed(1)
boston <- scale(Boston, center = T, scale = T)
train.ind = sample(1:nrow(boston), 0.8*nrow(boston))
boston.train = data.frame(boston[train.ind, ])
boston.test = data.frame(boston[-train.ind, ])
dim(boston.train)
```


\subsection{a)}
```{r,eval=TRUE,echo=TRUE}
library(leaps)
fwd_regfit <- regsubsets(medv~., data = boston.train, nvmax = 14, method = "forward" )
summary_fwd_regfit <- summary(fwd_regfit)
bck_regfit <- regsubsets(medv~., data = boston.train, nvmax = 14, method = "backward" )
summary_bck_regfit <- summary(bck_regfit)
par(mfrow=c(1,2))
plot(summary_fwd_regfit$rsq, xlab = "Number of Variables", ylab = "RSq fwd" ,type = "l")
plot(summary_bck_regfit$rsq, xlab = "Number of Variables", ylab = "RSq bck" ,type = "l")


```


\subsection{b)}

```{r,eval=TRUE,echo=TRUE}
number_predictors_selected <- 4
variables_fwd <- names(coef(fwd_regfit, id = number_predictors_selected))
variables_fwd
```
The four "predictors" from the forward stepwise selection are "rm" ,"dis", "ptratio" and "lstat".


\subsection{c)}

```{r,eval=TRUE,echo=TRUE}

library(glmnet)
set.seed(1)
x_train <- model.matrix(medv~., boston.train)[,-1]
y_train <- boston.train$medv

model_lass <- cv.glmnet(x_train, y_train, alpha =1, nfolds = 5)
plot(model_lass)


best_lambda <- model_lass$lambda.min
best_lambda

coef_lass <- coef(model_lass, s = model_lass$lambda.min )
coef_lass


```

(ii)
```{r,eval=TRUE,echo=TRUE}

best_lambda <- model_lass$lambda.min
best_lambda

```
Th best lambda value is given as:0.00315


(iii)
The fitted coefficients at the best $\lambda$ value is given by the function below. A plot for the coefficients value at different lambda is given as well. Indus is not a relevant coefficient at the lambda value with lowest MSE and the coefficient for age is very small small meaning that it is most likely significantly less important than the rest. This is possible to do for sclaed data. 

```{r,eval=TRUE,echo=TRUE}

coef_lass <- coef(model_lass, s = model_lass$lambda.min )
coef_lass

lambdas_to_try <- 10^seq(-7,1, length.out = 100)
res<- glmnet(x_train, y_train, alpha=1, lambda =lambdas_to_try ,standardize= FALSE)
plot(res, xvar= "lambda")
legend("bottomright", legend =colnames(x_train), cex =0.3)

```




\subsection{d)}
(i)True. Lasso is generally faster than step-wise especially when n(number of datapoints) and p(number of predictors) are very large and the number of relatively important predictors are small. This is because lasso can eliminate multiple predictors at once by increasing the value of lambda while stepwise can only eliminate it one by one. https://www.stat.cmu.edu/~ryantibs/papers/bestsubset.pdf

(ii) False.It is impossible for ridge regression to result in coefficients equal to zero. The coefficients approaches zero as lambda's value get extremely large. The coefficients can however become zero in Lasso regression.

(iii)False. Lasso is expected to perform better when there is a only relatively small number of important predictors and a significant proportion of unimportant coefficients that have very small or zero value. Ridge is expected to perform better when there is a high proportion of important predictors with predictors of all having roughly the same size.

(iv) True. The formula for Elastic Net is given as 

$\min _{\beta_{0}, \beta} \frac{1}{N} \sum_{i=1}^{N} w_{i} l\left(y_{i}, \beta_{0}+\beta^{T} x_{i}\right)+\lambda\left[(1-\alpha)\|\beta\|_{2}^{2} / 2+\alpha\|\beta\|_{1}\right]$

Where the $\alpha$ value can be varied between 0 and 1 to change the weighting between Ridge and Lasso.



\section{Problem 2}

```{r}
library(MASS)
set.seed(1)
# load a synthetic dataset
id <- "1CWZYfrLOrFdrIZ6Hv73e3xxt0SFgU4Ph" # google file ID
synthetic <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
# split into training and test sets
train.ind = sample(1:nrow(synthetic), 0.8 * nrow(synthetic)) 
synthetic.train = data.frame(synthetic[train.ind, ])
synthetic.test = data.frame(synthetic[-train.ind, ])
# show head(..)
# Y: response variable; X: predictor variable
head(synthetic)
```

\subsection{a)}
```{r,eval=TRUE,echo=TRUE}
library(pls)



pcr_model <- pcr(Y~., data =synthetic.train, scale =TRUE, validation ="CV")
validationplot(pcr_model, val.type ="MSEP")
title(sub ="PCR")

summary(pcr_model)


plsr_model<- plsr(Y~., data =synthetic.train, scale =TRUE, validation ="CV")
validationplot(plsr_model, val.type = "MSEP")
title(sub = "PLSR")

```



\subsection{b)}
The PCR method show an almost uniform reduction in MSEP with increasing number of principal components. On the other hand, PLSR showed a sharp drop in MSEP when moving from 0 to 1 component and from 3 to 4 components, with the MSEP approaching zero from 4 components and onwards. The main difference between the PCR and the PLSR method is that in PCR, the principal components are created without considering their significance to Y, while in PLSR the principal components are created such that each additional principal components is weighted to have less significance to the reponse variable.  


\section{Problem 3}

\subsection{a}

(i) True. The proof for this is pretty long and complicated. You can see it in the link

(ii) False. By increasing the number of cutpoints each step function will get more and more affected by the points within their range, and thus overfit.

(iii) False. The penalty term is \$\\int g''(t)\^2 dt\$.

(iv) True. With high \$k\$ the number more neighbors are needed in order to classify a point, thus the variance will be low and the bias will increase.

\subsection{b}

``` {r}
# Fit modelt
model <- gam(medv ~ rm + s(ptratio, k = 3) + poly(lstat, df = 2), data = boston.train)

# Plot model with training data
plot(model, boston.train)
```

\section{Problem 4}

\subsection{a)}

(i) False. The trees can handle interaction terms, but you can not specify them yourself. The tree will "find" them by its nature.

(ii) True. 

(iii) True.

(iv) True. It decides the number of "iterations" in the boosting algorithm.

\subsection{b)}


\subsection{c)}
First some given R code
```{r}
library(tidyverse)
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)

names(penguins) <- c("species","island","billL","billD","flipperL","mass","sex","year")

Penguins_reduced <- penguins %>%  
  dplyr::mutate(mass = as.numeric(mass),  
         flipperL = as.numeric(flipperL),
         year = as.numeric(year)) %>% 
  drop_na()

# We do not want "year" in the data (this will not help for future predictions)
Penguins_reduced <- Penguins_reduced[,-c(8)]


set.seed(4268)
# 70% of the sample size for training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)
train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```

(i)
```{r}
library(tree)

# Create the tree 
tree.penguin <- tree(species ~ .,
                     data = train,
                     split = 'gini')

# Plot the tree
plot(tree.penguin, type = 'uniform');text(tree.penguin)
```
(ii)
Now, apply 10-fold cross-validation 
```{r}
set.seed(123)

# Do 10-fold cross-validation
cv.penguin <- cv.tree(tree.penguin)

# Plot result from the 10-fold cross-validation
plot(cv.penguin)
```
(iii) 
From the above plot the optimal tree looks to be of size 4. 
```{r}

# Prune the tree according to the obseravtion in the above plot
prune_tree.penguin <- prune.misclass(tree.penguin, best = 4)

plot(prune_tree.penguin, type = 'uniform'); text(prune_tree.penguin)
```


```{r}
pred <- predict(prune_tree.penguin, test, type = 'class')
tab <- table(pred, test$species)
tab
```
Above you can see a table showing how good, and bad, the tree performs on the test data.

Now we will calculate the misclassification rate using this table.
```{r}
missclassification_rate <- round((1 - sum(diag(tab))/sum(tab)), 2)
print(paste('The missclassification rate is:', missclassification_rate))
```


\subsection{d)}
```{r}
library(gbm)
library(dismo)

# Fit boosted forest with initial parameters
boosted.penguin <- gbm(species~ .,
                       data = train,
                       n.trees = 100,
                       distribution = 'multinomial')

summary(boosted.penguin)
```

From the above plot you can see that "billL" and "flipperL" are the most influantial ones when it comes to predicting the species of the penguins.

Now we predict on the test data and calculate the misclassification rate:

```{r}
pred <- predict.gbm(boosted.penguin, test, type = 'response')

pred.penguin <- colnames(pred)[apply(pred,1,which.max)]

tab.boost <- table(pred.penguin, test$species)
tab.boost
```
Above you can see a table showing the right and wrong classifications of the boosted forest on the test data.

Now we will calculate the misclassification rate using this table.
```{r}
misclassification.rate.boost <- 1-sum(diag(tab.boost))/sum(tab.boost)
print(paste('The misclassification rate is:', misclassification.rate.boost))
```

\section{Problem 5}

\subsection{a)}
\begin{tabular}{c c c c}
i) & ii) & iii) & iv)\\
FALSE & TRUE & FALSE & TRUE\\
\end{tabular}

\subsection{b)}

```{r}

svc.cvtune <- function(kernel, paramgrid, k){
 ctrl <- tune.control(sampling="cross", cross=k, nrepeat = 1)
 cvtune.result <- tune(method=svm, species~., kernel=eval(kernel), 
      data=train, ranges=paramgrid, tunecontrol=ctrl)
 cvtune.result
}

k <- 5
svc.fit_and_eval <- function(kernel, paramgrid, k){
  res <- svc.cvtune(kernel, paramgrid, k)
  print(strrep("_", 60))
  print(paste(eval(kernel), "support vector classifier"))
  print(strrep("-", 60))
  print("Parameters:")
  print(c(res$best.parameters))
  
  model <- res$best.model
  pred <- as.factor(predict(model, test[-c(1)]))
  cm <- confusionMatrix(data=pred, reference=test$species)
  print(cm)
}

svc.fit_and_eval("linear", data.frame(cost=.1*1:20), k)
svc.fit_and_eval("radial", data.frame(cost=.1*1:20, gamma=.1*1:20), k)
```
Using a linear rather than radial kernel in the support vector classifier
yields slightly superior results on the test set. Thus, the linear kernel is
preferred.


\section{Problem 6}

\subsection{a)}
```{r}
id <- "1NJ1SuUBebl5P8rMSIwm_n3S8a7K43yP4" # google file ID
happiness <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),fileEncoding="UTF-8-BOM")
```

```{r}

cols = c('Country.name', 
         'Ladder.score',  # happiness score
         'Logged.GDP.per.capita',  
         'Social.support', 
         'Healthy.life.expectancy', 
         'Freedom.to.make.life.choices',
         'Generosity',  # how generous people are
         'Perceptions.of.corruption')
# We continue with a subset of 8 columns:
happiness = subset(happiness, select = cols)
rownames(happiness) <- happiness[, c(1)]
# And we create an X and a Y matrix
happiness.X = happiness[, -c(1, 2)]
happiness.Y = happiness[, c(1, 2)]
happiness.XY = happiness[, -c(1)]
# scale
happiness.X = data.frame(scale(happiness.X))
```


```{r, fig.height=9, fig.width=10,out.width='18cm'}
library(ggfortify)
pca_mat = prcomp(happiness.X, center=T, scale=T)

# Score and loadings plot:
autoplot(pca_mat, data = happiness.X, colour='Black',
         loadings = TRUE, loadings.colour = 'red',
         loadings.label = TRUE, loadings.label.size = 5, 
         label=T, label.size=4.5)
```
\subsubsection{i)}
We observe that the variables "Logged.GDP.per.capita", "Healthy.life.expenctancy"
and "Social.support" are highly correlated as they have nearly equal loadings on
PC1 and PC2.
The loading vectors for the variables "Freedom.to.make.like.choices" and 
"Perceptions.of.corruption" are nearly antiparallel, indicating that these
variables have a strong negative correlation.

\subsubsection{ii)}
Afghanistan appears to be clearly separated from the other countries in this
plot, and may be considered to be an outlier.


\subsection{b)}

\subsubsection{i)}
```{r}
rot <- pca_mat$rotation
loading.PC1 <- data.frame(variable=rownames(rot), loading=abs(rot[,1]**2))
ggplot(data=loading.PC1, aes(x=variable, y=loading)) +
  geom_col(color="white", fill="red") +
  coord_flip()
```
Applying an appropriate scaling to the values in the plot above 
results in the same values as in the plot generated by
the autoplot function (we are not sure how and why the autoplot
function scales loadings into different values than those given by 
pca_mat$rotation).

\subsubsection{ii)}
```{r}
plsr_model.1 <- plsR(formula=Ladder.score~., data=happiness.XY, 
                   scaleX=T, scaleY=T, nt=1)

```

\subsubsection{iii)}
```{r}
rot.PC1 <- plsr_model.1$ww
loading.PC1 <- data.frame(variable=rownames(rot.PC1), loading=abs(rot.PC1[,1]))
loading.PC1
ggplot(data=loading.PC1, aes(x=variable, y=loading)) +
  geom_col(color="white", fill="red") +
  coord_flip()
```

\subsubsection{iv)}
Based on the PLSR we see that the variables "Logged.GDP.per.capita", 
"Healthy.life.expectancy", and "Social.support" are the most important 
predictors for "Ladder.score".


\subsection{c)}

\subsection{d)}
\begin{tabular}{c c c c}
i) & ii) & iii) & iv)\\
FALSE & FALSE & TRUE & TRUE\\
\end{tabular}

\subsection{d)}
\subsubsection{i)}
```{r}
K = 3
km.out <- kmeans(happiness.X, K, iter.max = 10)
invalidclustering <- function(km.out){
  clust <- km.out$cluster
  scand <- clust[c("Norway", "Denmark", "Sweden", "Finland")]
  !((sd(scand) == 0) & (clust["United States"] != scand["Norway"]))
}

i <- 1
print(paste("iteration", i))
while(invalidclustering(km.out)){
  i <- i + 1
  print(paste("iteration", i))
  km.out <- kmeans(happiness.X, K, iter.max = 10)
}

autoplot(pca_mat, data = happiness.X, colour=km.out$cluster,
         label=F, label.size=5,
         loadings = F, loadings.colour = 'blue',
         loadings.label = F, loadings.label.size = 3)
```
K = 3 was the minimum parameter required to obtain the desired clustering conditions.

\subsubsection{ii)}
```{r}
clust <- km.out$cluster
clust[c("Norway", "Denmark", "Sweden", "Finland")]
clust["United States"]
mean_ladder <- sapply(1:3, function(i) mean(happiness.Y[names(clust[clust == i]),2]))
mean_ladder
```
We observe that the clustering algorithm places the scandinavian countries in 
one cluster, and the United States in another.
When computing the average value of the "Ladder.score" variable within each cluster,
we find that "Ladder.score" is largest for the cluster to which the 
scandinavian countries belong.
Thus, we conclude that the scandinavian countries belong to the "happiest"
cluster, and that the United States belong to the "medium happiness" cluster.

